{
    "crn_code": "20512",
    "season": "202201",
    "enrollment": {
        "enrolled": 97,
        "responses": 73,
        "declined": null,
        "no response": null
    },
    "ratings": [
        {
            "question_id": "YC402",
            "question_text": "Your level of engagement with the course was:",
            "options": [
                "very low",
                "low",
                "medium",
                "high",
                "very high"
            ],
            "data": [
                2,
                4,
                18,
                16,
                19
            ]
        },
        {
            "question_id": "YC404",
            "question_text": "What is your overall assessment of this course?",
            "options": [
                "poor",
                "fair",
                "good",
                "very good",
                "excellent"
            ],
            "data": [
                4,
                7,
                21,
                8,
                19
            ]
        },
        {
            "question_id": "YC405",
            "question_text": "The course was well organized to facilitate student learning.",
            "options": [
                "strongly disagree",
                "disagree",
                "neutral",
                "agree",
                "strongly agree"
            ],
            "data": [
                4,
                6,
                16,
                17,
                16
            ]
        },
        {
            "question_id": "YC406",
            "question_text": "I received clear feedback that improved my learning.",
            "options": [
                "strongly disagree",
                "disagree",
                "neutral",
                "agree",
                "strongly agree"
            ],
            "data": [
                6,
                6,
                14,
                16,
                15
            ]
        },
        {
            "question_id": "YC407",
            "question_text": "Relative to other courses you have taken at Yale, the level of intellectual challenge of this course was:",
            "options": [
                "much less",
                "less",
                "same",
                "greater",
                "much greater"
            ],
            "data": [
                1,
                0,
                4,
                24,
                28
            ]
        },
        {
            "question_id": "YC408",
            "question_text": "Relative to other courses you have taken at Yale, the workload of this course was:",
            "options": [
                "much less",
                "less",
                "same",
                "greater",
                "much greater"
            ],
            "data": [
                0,
                1,
                12,
                21,
                23
            ]
        }
    ],
    "narratives": [
        {
            "question_id": "YC401",
            "question_text": "What knowledge, skills, and insights did you develop by taking this course?",
            "comments": [
                "ideas about causal inference",
                "machine learning and causal inference",
                "Being exposed to causal inference in a different way.",
                "ML and Causal Inference",
                "casual knowledge",
                "I gained a knowledge of fundamental causal inference, such as the Neyman-Rubin causal model, causal estimands, Fisherian inference, matching, inverse probability weighting, and conditional average treatment effect (CATE) estimation. The latter half of the course focused on how modern machine learning models are integrated into the field of causal inference, such as random forests and boosting models as well as neural networks.",
                "Knowledge about causal inference and ML and how to implement them in R.",
                "this course is a mix of theory and applications, probably more theory than would be expected. lots of machine learning techniques, such as neural networks, random forests, etc. as well as methods for testing for heterogeneous treatment effects. balancing and matching covariates, estimating treatment effects, etc.",
                "ML methods in R",
                "I learned about casual inference (potential outcomes framework), propensity scores, random forests, neural networks, how to estimate CATEs.",
                "causal inference and R knowledge",
                "Causal inference (potential outcomes framework, average treatment effect, difference-in-means estimator), and ML techniques (decision trees, random forests, S-leaners, T-learners, X-learners, neural networks)",
                "causal inference and some knowledge about machine learning",
                "This was my first time really implementing random forests and neural nets. Also, I gained a much deeper understanding of propensity scores and matching than I had before, even though I had learned about those topics previously.",
                "The Professor offered a thorough but accessible survey of methods that all skillful economists in this data-intensive age should know. I learned both the fundamental theories in causal inference and implementable code.",
                "Self-learning",
                "ML",
                "I got insight into causal inference and major skills in machine learning.",
                "Honestly Nothing.",
                "Use of machine learning for causal inference purposes",
                "For your understanding: Causal inference is studying whether we can reasonable claim an effect was caused (or was a result of) a specified treatment.  The first half of the class was about how to evaluate causal inference and random experiments in a theoretical sense. The second half of the class implemented techniques to find treatment effects (e.g. neural networks, random forests).",
                "I learned so much! My goodness how to put it all in words.\r\n\r\nThe obvious, first, then:\r\n\r\nI'd known nothing about ML coming in, and I feel like I'm not a reasonably educated consumer of it, and I'm not scared of running my own. I also gained a lot of confidence in my ability TO code something difficult. I hadn't had such challenging coding problems before, so the practice of working through step-by-step in the homework was invaluable.\r\n\r\nI'd come in with an economics background but had never learned formally the Rubin causal model; I've found it extremely helpful as a jumping point when critiquing research in my field. Similarly, permutation inference as a tool was very helpful to learn.\r\n\r\nI hadn't used PSM before (economists are pretty skeptical of it, I think largely because most economics studies lack the identifying assumptions to make it a valid tool for inference), but I can see the merits and a lot of other fields use it commonly, so I was happy to have learned it.\r\n\r\nI also came in without having met one of the requirements (the last time I'd used R was 2012...), but was familiar with Matlab and Stata so thought I'd be able to handle it. The first homework was rough, but that really helped the learning curve.\r\n\r\nSo in terms of skills: a wonderful incentive to learn me a coding language that many people with whom I'm interested in collaborating use.\r\n\r\nIn terms of insights, I feel like this course really brought together a lot of what I'd learned before in a vague way, and made it concrete in my mind. I now feel like a lot of the tools that get tossed around in economics seminars are ones I actually possess and can use fairly confidently (e.g. code-your-own bootstrap).",
                "machine learning algorithms (neural nets + random forests mostly), causal framework",
                "Causal inference",
                "Machine learning tools.",
                "ML and causal inference",
                "Machine Learning",
                "Very bad course. Don't take. Unclear structure. Unclear homework. Unclear teaching.",
                "lots about causal inference and how it relates to machine learning",
                "Familiarity with concepts in causal inference, machine learning, and how to implement machine learning approaches in R",
                "Familiarity with Casual Inference Framework and assumptions, implementation of machine learning techniques for casual inference.",
                "Causal inference knowledge + some basic deep learning methods.(And how to use cluster :))",
                "causal ml"
            ]
        },
        {
            "question_id": "YC403",
            "question_text": "What are the strengths and weaknesses of this course and how could it be improved?",
            "comments": [
                "The psets could be more clearly stated, and closely related to the lectures.",
                "It is good.",
                "hard to understand the lecture",
                "The causal inference portion of this course is excellent: Jas is a leading researcher in the field and his knowledge of the material is incredibly deep and nuanced. This is why you come to Yale! \r\n\r\nThat being said, I think there needs to be some serious reflection on the teaching in this course. The course is very fast-paced and difficult material-wise, but it often feels as if you have little or no support. For example, although Jas holds weekly one-on-one office hours, you will feel belittled and put down by his answers, no matter how hard you try to understand the material. There were definitely many times throughout the semester where Jas' responses made me feel stupid and humiliated to the point that I questioned whether or not I was cut out to be in the course [and even the major]. Jas even bragged about how many students dropped the course numerous times throughout the courseit was as if he was rooting for us to fail.\r\n\r\nFor this reason, I would highly recommend anyone considering taking this course to have a solid group of friends with whom they can work on the homework assignments.",
                "Strengths: Cover the important topics about causal inference and ML\r\n\r\nWeakness: The class focuses more on examples and doesn't offer many clear explanations about the theory, which puts a significant amount of pressure on after-class self-learning. Also the slides are a little hard to follow.",
                "strengths- interesting and relevant content. weaknesses- prof is rather unorganized, takes a long time to receive feedback, assignments are difficult and dont get less time consuming over the semester even though the prof said they would, workload is high because of assignments and readings",
                "A fully applied setting after the first problem set, and exposes you to frontier questions in the field. Really tough course too, but you get out of it what you put in.",
                "The engagement of the class was not amazing because it was over Zoom. The material was challenging but you capsules tell that professor sekhon knew what he was talking about. The slides were good.",
                "I hope the course could spend some time on discussing some sample questions. It is really difficult to do the homework with only conceptual understanding.",
                "Strengths: the course comprehensively covers the foundation of causal inference before moving into machine learning methods that estimate causal effects.\r\nWeaknesses: The psets were worded somewhat ambiguously, though that may have been purposeful in order to help us learn. More office hours per week would have been useful as well.",
                "The only improvement to this class would be if it was in person. The teaching staff, course material, organization, and material were exceptional.",
                "Prof should teach more code in the lecture time",
                "An in-person course would have been helpful",
                "While the material is interesting in theory, the class overall is quite badly taught. The attitudes of the lecturer and graduate teaching staff were actively hostile towards questions and often made students feel belittled for not knowing material that wasnt well taught in the first place. Lectures were disorganized and didnt cover content necessary to complete problem sets that often had typos and lacked clarity. Jas clearly prefers having students who have high levels of understanding in his class but has no tolerance for any variation in skill level. Also, different office hours sessions would give different answers to the same questions frequently, emphasizing the lack of clarity in teaching. I would stay clear from this course, as it rushes both the causal inference and machine learning portions, leading to not really knowing how to do either well, unless you have a background in both. The problem sets and exams reflected a level of material that far surpassed what was taught in class, and it felt like we had no resources or support to bridge this gap. Additionally, lots of course material is directly related to Jas own research, meaning there are no external resources to consult.",
                "It's hard.",
                "This course was completely online and that was a major weakness of the course. This course could be massively improved by strictly in person instruction. I liked the office hours but thought there could have been more.",
                "The teaching was aweful, the TA support was lacking, the HWK was barely connect to the course, the HWK was grotesque and excessive, it was all generally bad",
                "Strengths: Very interesting look into use of ML in causal inference & the instructor was very encouraging in soliciting student engagement\r\nWeakness: All courses conducted online - would have preferred hybrid format",
                "Strengths: Reasonable amount of work that all facilitated learning. Pset weren't busy work and helped learn the material. Professor was very open to questions and very responsive to questions over Ed Discussion. Great balance of theory and application for the two halves of the course. \r\n\r\nWeaknesses: It was all online; we've moved on from online classes",
                "Strengths: it really forces us to learn independently as we struggle with all those homeworks, but Jas's lectures complement the readings nicely and pull out the important aspects.\r\n\r\nWeaknesses: I thought this course was nigh perfect. I've been describing it as \"killing me sweetly.\" The homework does take a long time, but it's clear from the outset that it's going to require a lot of effort, and I think students can figure it out and self-select.",
                "I liked how the instructors facilitated setting up homework groups",
                "Too difficult",
                "The material in the course is really useful, but it would be really helpful if the professor could give more concrete examples in his lecture in stead of talking only about conceptual things.",
                "Very bad course. Don't take. Unclear structure. Unclear homework. Unclear teaching.",
                "strengths great solid intro and theoretical, applied, and computational at the same time. weakness: it was a lot because of that, wish it was a little less intense.",
                "Jas is a very effective lecturer, and explains things well and clearly by and large. The homework assignments are certainly engaging and challenging, and the study groups were really helpful in tacking the problem sets.\r\n\r\nI think the course could spend much more time right at the beginning of the course giving an overview of what causal inference and machine learning are, what they're used for, etc, before diving into the details. I definitely met the prereqs for the course but had never learned what causal inference was before, and only knew machine learning from news articles and Youtube videos, so a little framing of the big picture would have been helpful.\r\n\r\nSpeaking of pre-reqs, Jas has a tendency to respond negatively when someone asks a question about material covered in prerequisite courses. This could be greatly improved upon. Nobody remembers 100% of what has been covered in pre-requisite courses, so replying to straightforward student questions about pre-requisite content with statements like \"that's something you should know from the pre-requisites, so if you don't you should consider whether you're ready for this course\" (a paraphrase from one of the first few lectures) is damaging. It discourages students from asking questions, for fear that they too will feel belittled for something they may have learned and forgot. It directly discourages students from learning because Jas goes out of his way to avoid refreshing our memories on those topics (the student asking the question is almost certainly not the only one who forgot about some material from some semesters or years ago). And it also just sets a negative tone between Jas as the students. Jas can and should respond with a brief explanation refreshing our memories and leave it at that. Once he's told us once in the first lecture what the pre-reqs are and that we should make sure we meet them, there's no need to re-state them every time someone asks a question about content that may be covered in a previous class.\r\n\r\nFinally, I think the course could be improved by spending much more time in lecture talking about how to actually implement some of the machine learning algorithms in R. Even just having a list of R libraries and functions that are commonly used to implement them in the slides, although it would also be helpful to dig in a little more depth on what arguments to pass to the most-common functions. The multivariate stats class taught by Jonathan Reuning-Scherer here does this exceptionally well, for instance. This would help greatly decrease the amount of time that we spend on the problem sets, since often the first half of it is just googling around or reading the class forum just to figure out what R package we should be reading the documentation for in the first place. It would also really help us actually learn best practices for implementation of machine learning, rather than just the concepts of how these things are implemented.",
                "Material is very well presented (slide decks are great). Homeworks were laborious but effective--my difficulty with them is probably related to my pre-requisites being a little stale (old). Grading was generous and material was very well presented.",
                "This course is awesome. Causal inference is a relatively new subject, and sometimes I was so confused during the process of learning. However, TAs are very helpful, most problems could be solved.",
                "great let"
            ]
        },
        {
            "question_id": "YC409",
            "question_text": "Would you recommend this course to another student? Please explain.",
            "comments": [
                "Not really. Remote teaching makes the difficult content even harder to follow. The lectures were not very helpful to the psets, which could be a struggle.",
                "no",
                "its an interesting elective but needs to be better organized. the first several psets had numerous typos and inconsistencies on them that made them very difficult and time consuming, and it was frustrating to constantly be checking the ed discussion page and canvas announcements for clarifications on the psets. the prof is also rude and comes across as condescending, but is very knowledgable in his field.",
                "Only if you have the fundamentals in probability, math, and econometrics.",
                "Yes!! By far one of the most practical classes in the stats department, and lots of opportunities to read up on non-ancient stats literature which I thought was lots of fun",
                "Yes, but only if you really need an S&DS elective and theres not other good options.",
                "Yes. It challenges me, as an non statistics student, to think out of the box. i feel really excited to think about the unobserved outcomes, and try to understand how the same treatment can have different effects.",
                "Yes, as it is a comprehensive course that covers some of the most novel methods in ML for conditional average treatment effect estimation (many of them authored by Jas). But, be prepared to put in a lot of work for psets that are due every other week.",
                "Yes.",
                "I absolutely would recommend this class to any researcher that will be doing empirical work with large data sets or designing experiments. It contains foundational knowledge that will serve the student well.",
                "If you have knowledge about machine learning, you can take it. otherwise, do not take it",
                "No, there are much better courses to learn machine learning and the theory sections were badly taught. The problem sets and exams are extremely difficult, and while grading is relatively generous, you receive no support to do well on them. The lectures have almost nothing to do with problem sets or exams.",
                "Yes",
                "I would definitely recommend this course to other people because it was really insightful and engaging.",
                "No. It was really bad",
                "Yes, especially to students interested in causal inference. I learned a lot from the course!",
                "Yes, but keep in mind the two halves of the course. Material in the firs half is very theory and conceptual whereas the implementation and the coding (arguably more fun and applicable) comes later.",
                "Absolutely, although it depends on their willingness to put in effort. I don't know how much auditors would get out of it. I learned so much precisely from putting in all that time. \r\n\r\nIn short, it depends on what the student wants to get. If they're looking for a little intro to ML, better just go on youtube. If they'd really like to learn and incorporate these skills into other areas of study, then absolutely.",
                "Yes, i think if you have background knowledge of causal and R coding it is fun and interesting. If you dont have those things, it could be quite challenging i think.",
                "Yes",
                "Yes, I would",
                "Very bad course. Don't take. Unclear structure. Unclear homework. Unclear teaching.",
                "yes but lots of work and hours spent",
                "You learn a lot of causal inference probability theory, and a fair amount of machine learning implementation in R. However, it's a ton of work, and especially if you're primarily interested in the machine learning (like I was) you'll spend ~half the course only doing causal inference theory.",
                "Yes! This course is a wonderful combination of conceptual background and application. I learned so much and I am excited to apply these techniques to my own research. It is challenging and time-consuming, but definitely worth the trouble.",
                "Yes!",
                "yes"
            ]
        }
    ],
    "extras": {
        "title": "S&DS 317 01/S&DS 517 01 - MachineLearningCausalInference"
    },
    "sentiment_info": {
        "YC401": {
            "sentiment_labels": [
                "neutral",
                "neutral",
                "neutral",
                "neutral",
                "neutral",
                "neutral",
                "neutral",
                "neutral",
                "neutral",
                "neutral",
                "neutral",
                "neutral",
                "neutral",
                "positive",
                "positive",
                "neutral",
                "neutral",
                "neutral",
                "neutral",
                "neutral",
                "neutral",
                "positive",
                "neutral",
                "neutral",
                "neutral",
                "neutral",
                "neutral",
                "negative",
                "neutral",
                "neutral",
                "neutral",
                "neutral",
                "neutral"
            ],
            "sentiment_scores": [
                0.6364291310310364,
                0.6393197178840637,
                0.7622857689857483,
                0.6305574178695679,
                0.647676408290863,
                0.5660036206245422,
                0.7178916335105896,
                0.5519864559173584,
                0.6476648449897766,
                0.7281615734100342,
                0.7318194508552551,
                0.7298847436904907,
                0.5947189927101135,
                0.7721866369247437,
                0.624805748462677,
                0.5139861106872559,
                0.541739821434021,
                0.6686264872550964,
                0.558302640914917,
                0.717269778251648,
                0.8368784189224243,
                0.9256287813186646,
                0.7234601974487305,
                0.6501786112785339,
                0.619411826133728,
                0.5925275683403015,
                0.5303758978843689,
                0.9145326018333435,
                0.5807499885559082,
                0.6041361689567566,
                0.7333770990371704,
                0.5395396947860718,
                0.6481762528419495
            ],
            "sentiment_counts": {
                "neutral": 29,
                "positive": 3,
                "negative": 1
            },
            "sentiment_distribution": {
                "neutral": 0.8787878787878788,
                "positive": 0.09090909090909091,
                "negative": 0.030303030303030304
            },
            "sentiment_overall": [
                "neutral",
                29
            ]
        },
        "YC403": {
            "sentiment_labels": [
                "neutral",
                "positive",
                "neutral",
                "positive",
                "neutral",
                "neutral",
                "neutral",
                "neutral",
                "neutral",
                "neutral",
                "positive",
                "neutral",
                "neutral",
                "negative",
                "negative",
                "negative",
                "negative",
                "positive",
                "positive",
                "neutral",
                "positive",
                "negative",
                "neutral",
                "negative",
                "neutral",
                "positive",
                "positive",
                "positive",
                "neutral"
            ],
            "sentiment_scores": [
                0.8179752230644226,
                0.6213170886039734,
                0.5294250845909119,
                0.7517662644386292,
                0.8097284436225891,
                0.5981135964393616,
                0.6521674394607544,
                0.4601716995239258,
                0.5362591743469238,
                0.8232170939445496,
                0.8743626475334167,
                0.7573957443237305,
                0.6505671739578247,
                0.7205860018730164,
                0.5318071842193604,
                0.48564913868904114,
                0.8922086358070374,
                0.6932032108306885,
                0.7745498418807983,
                0.608526885509491,
                0.5949577689170837,
                0.548590362071991,
                0.508313775062561,
                0.9159234762191772,
                0.5287125706672668,
                0.5531744360923767,
                0.7420596480369568,
                0.8655439019203186,
                0.5801442265510559
            ],
            "sentiment_counts": {
                "neutral": 14,
                "positive": 9,
                "negative": 6
            },
            "sentiment_distribution": {
                "neutral": 0.4827586206896552,
                "positive": 0.3103448275862069,
                "negative": 0.20689655172413793
            },
            "sentiment_overall": [
                "neutral",
                14
            ]
        },
        "YC409": {
            "sentiment_labels": [
                "negative",
                "negative",
                "negative",
                "neutral",
                "positive",
                "neutral",
                "positive",
                "neutral",
                "neutral",
                "positive",
                "neutral",
                "negative",
                "neutral",
                "positive",
                "negative",
                "positive",
                "neutral",
                "positive",
                "positive",
                "neutral",
                "neutral",
                "negative",
                "negative",
                "neutral",
                "positive",
                "neutral",
                "neutral"
            ],
            "sentiment_scores": [
                0.7775769233703613,
                0.5286087393760681,
                0.7387651801109314,
                0.8502810001373291,
                0.9673207998275757,
                0.6010003685951233,
                0.8887911438941956,
                0.6605064868927002,
                0.8615380525588989,
                0.8573427796363831,
                0.7684352397918701,
                0.8924696445465088,
                0.895555853843689,
                0.9454582929611206,
                0.9112942814826965,
                0.9342607855796814,
                0.6373738050460815,
                0.5500622391700745,
                0.5367322564125061,
                0.895555853843689,
                0.8346066474914551,
                0.9206342101097107,
                0.5126765966415405,
                0.6870934963226318,
                0.9752352833747864,
                0.7555313110351562,
                0.8230147957801819
            ],
            "sentiment_counts": {
                "negative": 7,
                "neutral": 12,
                "positive": 8
            },
            "sentiment_distribution": {
                "negative": 0.25925925925925924,
                "neutral": 0.4444444444444444,
                "positive": 0.2962962962962963
            },
            "sentiment_overall": [
                "neutral",
                12
            ]
        },
        "final_label": "neutral",
        "final_count": 55,
        "final_proportion": 0.6179775280898876,
        "final_counts": {
            "neutral": 55,
            "positive": 20,
            "negative": 14
        },
        "final_distribution": {
            "neutral": 0.6179775280898876,
            "positive": 0.2247191011235955,
            "negative": 0.15730337078651685
        }
    }
}