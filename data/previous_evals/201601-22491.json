{
 "crn_code": "22491",
 "season": "201601",
 "legacy_coursetable_course_id": 37014,
 "enrollment": {
  "enrolled": 11,
  "responses": null,
  "declined": null,
  "no response": null
 },
 "ratings": [
  {
   "question_id": "GS004",
   "question_text": "Work Load",
   "options": [
    "light",
    "appropriate",
    "heavy"
   ],
   "data": [
    0,
    3,
    1
   ]
  },
  {
   "question_id": "GS005",
   "question_text": "Pace",
   "options": [
    "slow",
    "appropriate",
    "fast"
   ],
   "data": [
    0,
    3,
    1
   ]
  },
  {
   "question_id": "GS006",
   "question_text": "Difficulty",
   "options": [
    "easy",
    "appropriate",
    "hard"
   ],
   "data": [
    0,
    4,
    0
   ]
  },
  {
   "question_id": "GS007",
   "question_text": "Preparation for Departmental Requirements",
   "options": [
    "not helpful",
    "somewhat helpful",
    "very helpful",
    "not applicable"
   ],
   "data": [
    0,
    2,
    2,
    0
   ]
  },
  {
   "question_id": "GS008",
   "question_text": "What is your overall assessment of this course?",
   "options": [
    "poor",
    "below average",
    "good",
    "very good",
    "excellent"
   ],
   "data": [
    0,
    0,
    2,
    0,
    2
   ]
  }
 ],
 "narratives": [
  {
   "question_id": "GS003",
   "question_text": "How would you summarize AFAM 505 01\/AMST 643 01 for a fellow student? Would you recommend AFAM 505 01\/AMST 643 01 to another student? Why or why not?",
   "comments": [
    "Recommend. Very helpful in machine learning area.",
    "Convex optimization is a course highly applicable to many areas of engineering and science. But I would especially like to recommend the course to the graduate students in engineering and statistics who work closely with regressions and regression related technologies (e.g. machine learning, pattern recognition) For those students, the crown jewel of the course may be the subgradient method. The subgradient method is the key to understanding the complexities and solutions of LASSO, and other penalty based method of regression. Even though the subgradient method takes the cake in practicality and imaginativeness, other topics in the course are quite interesting as well. Jensen's inequality follows effortlessly once the convex inequalities are understood, and Newtons method turns out to be another convex optimization method."
   ]
  },
  {
   "question_id": "GS009",
   "question_text": "How did this course deepen your knowledge of your discipline\/profession? To what extent did this course contribute to your dissertation\/research?",
   "comments": [
    "Very helpful in my research.",
    "The course unlocked the key for me to understand the complexities and solutions of penalty methods of regression. This course is a very good stepping stone for me to tackle on more advanced and different types of regression penalty methods."
   ]
  }
 ],
 "extras": {
  "subject": "ENAS",
  "number": "530",
  "section": 1
 }
}